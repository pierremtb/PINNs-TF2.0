{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from schrodingerutil import prep_data, plot_inf_cont_results\n",
    "from logger import Logger\n",
    "from neuralnetwork import NeuralNetwork\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "\n",
    "# HYPER PARAMETERS\n",
    "\n",
    "eqnPath = \"1dcomplex-schrodinger\"\n",
    "if len(sys.argv) > 1:\n",
    "    with open(sys.argv[1]) as hpFile:\n",
    "        hp = json.load(hpFile)\n",
    "else:\n",
    "    hp = {}\n",
    "    # Data size on the initial condition solution\n",
    "    hp[\"N_0\"] = 50\n",
    "    # Collocation points on the boundaries\n",
    "    hp[\"N_b\"] = 50\n",
    "    # Collocation points on the domain\n",
    "    hp[\"N_f\"] = 20000\n",
    "    # DeepNN topology (2-sized input [x t], 4 hidden layer of 100-width, 2-sized output [u, v])\n",
    "    hp[\"layers\"] = [2, 100, 100, 100, 100, 2]\n",
    "    # Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "    hp[\"tf_epochs\"] = 200\n",
    "    hp[\"tf_lr\"] = 0.05\n",
    "    hp[\"tf_b1\"] = 0.99\n",
    "    hp[\"tf_eps\"] = 1e-1\n",
    "    # Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "    hp[\"nt_epochs\"] = 0\n",
    "    hp[\"nt_lr\"] = 1.2\n",
    "    hp[\"nt_ncorr\"] = 50\n",
    "    hp[\"log_frequency\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5827d",
   "metadata": {
    "title": "DEFINING THE MODEL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SchrodingerInformedNN(NeuralNetwork):\n",
    "    def __init__(self, hp, logger, X_f, tb, ub, lb):\n",
    "        super().__init__(hp, logger, ub, lb)\n",
    "\n",
    "        X_lb = np.concatenate((0*tb + lb[0], tb), 1)  # (lb[0], tb)\n",
    "        X_ub = np.concatenate((0*tb + ub[0], tb), 1)  # (ub[0], tb)\n",
    "        self.X_lb = self.tensor(X_lb)\n",
    "        self.X_ub = self.tensor(X_ub)\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.x_f = self.tensor(X_f[:, 0:1])\n",
    "        self.t_f = self.tensor(X_f[:, 1:2])\n",
    "\n",
    "    # Decomposes the multi-output into the complex values and spatial derivatives\n",
    "    def uvx_model(self, X):\n",
    "        x = X[:, 0:1]\n",
    "        t = X[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            tape.watch(t)\n",
    "            Xtemp = tf.concat([x, t], axis=1)\n",
    "\n",
    "            h = self.model(Xtemp)\n",
    "            u = h[:, 0:1]\n",
    "            v = h[:, 1:2]\n",
    "\n",
    "        u_x = tape.gradient(u, x)\n",
    "        v_x = tape.gradient(v, x)\n",
    "        del tape\n",
    "\n",
    "        return u, v, u_x, v_x\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the two inputs we’ll need later, x and t\n",
    "            tape.watch(self.x_f)\n",
    "            tape.watch(self.t_f)\n",
    "            # Packing together the inputs\n",
    "            X_f = tf.concat([self.x_f, self.t_f], axis=1)\n",
    "\n",
    "            # Getting the prediction\n",
    "            u, v, u_x, v_x = self.uvx_model(X_f)\n",
    "\n",
    "        # Getting the other derivatives\n",
    "        u_xx = tape.gradient(u_x, self.x_f)\n",
    "        v_xx = tape.gradient(v_x, self.x_f)\n",
    "        u_t = tape.gradient(u, self.t_f)\n",
    "        v_t = tape.gradient(v, self.t_f)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "\n",
    "        h2 = (u**2 + v**2)\n",
    "        f_u = u_t + 0.5*v_xx + h2*v\n",
    "        f_v = v_t - 0.5*u_xx - h2*u\n",
    "\n",
    "        return f_u, f_v\n",
    "\n",
    "    def loss(self, uv, uv_pred):\n",
    "        u0 = uv[:, 0:1]\n",
    "        v0 = uv[:, 1:2]\n",
    "        u0_pred = uv_pred[:, 0:1]\n",
    "        v0_pred = uv_pred[:, 1:2]\n",
    "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = \\\n",
    "                self.uvx_model(self.X_lb)\n",
    "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = \\\n",
    "                self.uvx_model(self.X_ub)\n",
    "        f_u_pred, f_v_pred = self.f_model()\n",
    "\n",
    "        mse_0 = tf.reduce_mean(tf.square(u0 - u0_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(v0 - v0_pred))\n",
    "        mse_b = tf.reduce_mean(tf.square(u_lb_pred - u_ub_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(v_lb_pred - v_ub_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(u_x_lb_pred - u_x_ub_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(v_x_lb_pred - v_x_ub_pred))\n",
    "\n",
    "        mse_f = tf.reduce_mean(tf.square(f_u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_v_pred))\n",
    "\n",
    "        tf.print(f\"mse_0 {mse_0}    mse_b {mse_b}    mse_f    {mse_f}\")\n",
    "        return mse_0 + mse_b + mse_f\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        h_pred = self.model(X_star)\n",
    "        u_pred = h_pred[:, 0:1]\n",
    "        v_pred = h_pred[:, 1:2]\n",
    "        return u_pred.numpy(), v_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f49720",
   "metadata": {
    "title": "TRAINING THE MODEL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(eqnPath, \"data\", \"NLS.mat\")\n",
    "x, t, X, T, Exact_u, Exact_v, Exact_h, \\\n",
    "    X_star, u_star, v_star, h_star, X_f, \\\n",
    "    ub, lb, tb, x0, u0, v0, X0, H0 = prep_data(\n",
    "        path, hp[\"N_0\"], hp[\"N_b\"], hp[\"N_f\"], noise=0.0)\n",
    "\n",
    "# Creating the model\n",
    "logger = Logger(hp)\n",
    "\n",
    "pinn = SchrodingerInformedNN(hp, logger, X_f, tb, ub, lb)\n",
    "\n",
    "# Defining the error function for the logger\n",
    "\n",
    "\n",
    "def error():\n",
    "    u_pred, v_pred = pinn.predict(X_star)\n",
    "    h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    return np.linalg.norm(h_star - h_pred, 2) / np.linalg.norm(h_star, 2)\n",
    "\n",
    "\n",
    "logger.set_error_fn(error)\n",
    "\n",
    "# Training the PINN\n",
    "pinn.fit(x0, tf.concat([u0, v0], axis=1))\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, v_pred = pinn.predict(X_star)\n",
    "h_pred = np.sqrt(u_pred**2 + v_pred**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0687004",
   "metadata": {
    "title": "PLOTTING"
   },
   "outputs": [],
   "source": [
    "plot_inf_cont_results(X_star, u_pred, v_pred, h_pred, Exact_h, X, T, x, t, ub, lb, x0, tb,\n",
    "                      save_path=eqnPath, save_hp=hp)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
